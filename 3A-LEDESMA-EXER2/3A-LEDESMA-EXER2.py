# -*- coding: utf-8 -*-
"""3A-LEDESMA-EXER2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RKH2qLL1WQ88DIiHHWXbr5Wt9HtTrc54

## **Exercise 1**: Introduction to Probability Theory in AI

---

**Objective**: Understand the fundamentals of probability theory and its application in AI for decision-making under uncertainty.

1. Basic Probability Calculation: Write Python functions that calculate the following:
"""

def joint_probability(p_A, p_B):
  return p_A*p_B

def marginal_probability(p_A, p_B):
  return p_A + p_B - joint_probability(p_A, p_B)

def conditional_probability(p_B_given_A, p_A, p_B):
  return (p_B_given_A * p_A) / p_B

"""### **Assessment Task 1**: Basic Probability Calculations

---

1. Start with a Coding Task:
"""

def joint_probability(p_A, p_B):
  return p_A*p_B

def marginal_probability(p_A, p_B):
  return p_A + p_B - joint_probability(p_A, p_B)

def conditional_probability(p_B_given_A, p_A, p_B):
  return (p_B_given_A * p_A) / p_B

"""2. Interactive Question:"""

p_A = 0.3
p_B = 0.4
p_B_given_A = 0.8

print(f"Joint Probability: {joint_probability(p_A, p_B)}")
print(f"Marginal Probability: {marginal_probability(p_A, p_B)}")
print(f"Conditional Probability: {conditional_probability(p_B_given_A, p_A, p_B)}")

"""## **Exercise 2**: Decision-Making Under Uncertainty

---

**Objective**: Apply probability theory to make decisions in uncertain environments, such as AI systems that need to weigh the outcomes of different choices.
"""

import numpy as np

def simulate_decision(num_simulations, p_success, reward_success, reward_failure):
  outcomes = []
  for _ in range(num_simulations):
    if np.random.rand() < p_success:
      outcomes.append(reward_success)
    else:
      outcomes.append(reward_failure)
  return np.mean(outcomes)


p_success = 0.7
reward_success = 1000
reward_failure = -500

average_outcome = simulate_decision(1000, p_success, reward_success, reward_failure)
print(f"Expected Value of Decision: {average_outcome}")

"""### **Assessment Task 2**: Decision-Making Under Uncertainty

---

1. Interactive Decision Simulation
"""

import numpy as np

def simulate_decision(probability_of_success, iterations=1000):
    success_reward = 1000
    failure_penalty = -500

    results = []

    for _ in range(iterations):
        if np.random.rand() < probability_of_success:
            results.append(success_reward)
        else:
            results.append(failure_penalty)

    average_return = np.mean(results)
    return average_return, results

"""2. Interactive Task"""

def run_simulation():
    try:
        probability_of_success = float(input("Enter the probability of success (0 to 1): "))
        if not (0 <= probability_of_success <= 1):
            raise ValueError("Probability must be between 0 and 1.")
    except ValueError as e:
        print(f"Invalid input: {e}")
        return

    average_return, results = simulate_decision(probability_of_success)

    print(f"Average return after 1000 iterations: ${average_return:.2f}")
    print(f"Total profit/loss after 1000 iterations: ${sum(results):.2f}")

if __name__ == "__main__":
    run_simulation()

"""**Feedback**

- Probability distributions play a critical role in decision-making. The expected value guides the decision, but variance and risk must also be considered.
- A high variance in outcomes means there's more uncertainty, even if the expected return is positive.

## **Exercise 3**: Applying Probability Theory in AI for Diagnosis

---

**Objective**: Use probabilistic reasoning to model uncertainty in a real-world scenario such as medical diagnosis.
"""

def bayesian_inference(prior, likelihood_positive_given_disease, likelihood_positive):
  posterior = (likelihood_positive_given_disease * prior) / likelihood_positive
  return posterior


prior_disease = 0.01
likelihood_positive_given_disease = 0.9
likelihood_positive = 0.05

posterior_disease = bayesian_inference (prior_disease, likelihood_positive_given_disease, likelihood_positive)
print(f"Posterior Probability of Disease Given Positive Test: {posterior_disease}")

"""### **Assessment Task 3**: Bayesian Inference

---

1. Interactive Bayesian Model
"""

def bayesian_inference(prior_H, likelihood_E_given_H, false_positive_rate, prior_not_H):
    evidence_E = (likelihood_E_given_H * prior_H) + (false_positive_rate * prior_not_H)

    posterior_H_given_E = (likelihood_E_given_H * prior_H) / evidence_E

    return posterior_H_given_E

"""2. Interactive Scenario"""

prior_H = 0.01
likelihood_E_given_H = 0.9
false_positive_rate = 0.05
prior_not_H = 1 - prior_H

posterior_H_given_E = bayesian_inference(prior_H, likelihood_E_given_H, false_positive_rate, prior_not_H)

print(f"Probability of having the disease given a positive test result: {posterior_H_given_E:.4f}")

"""## **Exercise 4**: Probability Distribution in AI

---

**Objective**: Work with probability distributions that are key in AI for representing uncertainty.
"""

p_purchase_given_cart = 0.4
p_cart = 0.3

p_purchase = conditional_probability(p_purchase_given_cart, p_cart, 1)
print(f"Probability of Purchase: {p_purchase}")

"""### **Assessment Task 4**: Real-World Scenario

---

1. Interactive Real-World Problem
"""

def rain_prediction(humidity_level, cloud_cover_level):
    p_humidity = humidity_level
    p_cloud = cloud_cover_level

    p_rain = 0.3
    p_humidity_given_rain = 0.8
    p_cloud_given_rain = 0.7

    return (p_humidity_given_rain * p_cloud_given_rain * p_rain) / (p_humidity * p_cloud)

"""2. Scenario Simulation"""

humidity_level = float(input("Enter the humidity level (0 to 1): "))
cloud_cover_level = float(input("Enter the cloud cover level (0 to 1): "))

rain_prob = rain_prediction(humidity_level, cloud_cover_level)
print(f"\nProbability of rain: {rain_prob:.2f}")

"""**Feedback**

Increasing Humidity
- As humidity increases, the probability of rain typically increases because higher humidity is more likely when it rains.

Increasing Cloud Cover
- Similarly, increased cloud cover generally corresponds to a higher chance of rain.

## **Exercise 5**: Real-World Application of Probability in AI

---

**Objective**: Apply probability theory to real-world AI scenarios, such as predicting customer behavior or making decisions in uncertain markets.
"""

import numpy as np
import matplotlib.pyplot as plt

n_trials = 1000
p_head = 0.5
binomial_distribution = np.random.binomial(n = 1, p = p_head, size = n_trials)

plt.hist(binomial_distribution, bins = 2)
plt.title('Binomial Distribution (Coin Flips)')
plt.show()

"""### **Assessment Task 5**: Probability Distributions Visualization

---

1. Interactive Visualization Task
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm, binom

sns.set(style="whitegrid")

# Generating and plotting Binomial Distribution
def plot_binomial_distribution(n, p):
    x = np.arange(0, n + 1)
    binom_probs = binom.pmf(x, n, p)

    plt.figure(figsize=(10, 6))
    plt.bar(x, binom_probs, color='skyblue')
    plt.title(f'Binomial Distribution: n={n}, p={p}')
    plt.xlabel('Number of successes')
    plt.ylabel('Probability')
    plt.show()

# Generating and plotting Normal Distribution
def plot_normal_distribution(mu, sigma):
    x = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)
    plt.figure(figsize=(10, 6))
    plt.plot(x, norm.pdf(x, mu, sigma), color='red')
    plt.title(f'Normal Distribution: μ={mu}, σ={sigma}')
    plt.xlabel('Value')
    plt.ylabel('Probability Density')
    plt.show()

plot_binomial_distribution(10, 0.5)  # Binomial Distribution with n=10, p=0.5
plot_normal_distribution(0, 1)

"""2. Interactive Question  
- **Task**: Simulate 1000 coin flips using a binomial distribution and visualize the outcomes.
"""

def simulate_coin_flips(trials, prob_head, num_simulations):
    outcomes = np.random.binomial(trials, prob_head, num_simulations)

    plt.figure(figsize=(10, 6))
    sns.histplot(outcomes, bins=range(trials+2), kde=False, color='skyblue')
    plt.title(f'Simulation of {num_simulations} Coin Flips: n={trials}, p={prob_head}')
    plt.xlabel('Number of heads')
    plt.ylabel('Frequency')
    plt.show()

simulate_coin_flips(10, 0.5, 1000)

"""**Follow-Up** - How does increasing the number of trials or changing the probability of heads affect the distribution?
-
Increasing the number of trials in a probability experiment, like flipping a coin, makes the distribution of results more predictable and closer to the expected probability. For example, if the probability of heads is 0.5, more trials will show the outcomes clustering around 50% heads. Changing the probability of heads alters the distribution's center. If the probability of heads is more than 0.5, the distribution will shift towards more heads; if less, it shifts towards more tails.

3. Interactive Graph
"""

import ipywidgets as widgets
from IPython.display import display

# Interactive widget for binomial distribution
def interactive_binomial(trials, p):
    simulate_coin_flips(trials, p, 1000)

# Setting up sliders
trial_slider = widgets.IntSlider(min=1, max=100, step=1, value=10, description='Trials')
prob_slider = widgets.FloatSlider(min=0, max=1, step=0.01, value=0.5, description='p(head)')

# Display interactive widget
display(widgets.interactive(interactive_binomial, trials=trial_slider, p=prob_slider))

"""**Feedback**

- Increasing trials leads to a broader distribution with more possible outcomes.

- Changing p(head) shifts the peak of the distribution:
1. A p(head) of 0.5 is centered around n/2.
2. A p(head) closer to 1 results in most trials resulting in heads.
"""